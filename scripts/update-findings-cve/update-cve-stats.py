#!/usr/bin/env python3

"""
Update CVE statistics for findings.json using vulnx.
This script queries vulnx for CVE data and updates findings.json with security stats.

Usage:
  python3 scripts/update-findings-cve/update-cve-stats.py
  python3 scripts/update-findings-cve/update-cve-stats.py --resume-from frontend.backbone

Environment variables:
  RATE_LIMIT_DELAY: Delay in seconds between findings (default: 2.0)
"""

import json
import subprocess
import sys
import re
import time
import os
from datetime import datetime, timezone
from pathlib import Path

FINDINGS_FILE = Path("pkg/webexposure/findings/findings.json")

# Get delay from environment or use default
RATE_LIMIT_DELAY = float(os.environ.get('RATE_LIMIT_DELAY', '2.0'))
QUERY_DELAY = float(os.environ.get('QUERY_DELAY', '0.5'))


def check_dependencies():
    """Check if required tools are installed."""
    try:
        result = subprocess.run(["vulnx", "version"], capture_output=True, check=True, text=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("Error: vulnx is not installed.")
        print("Install it with: go install github.com/projectdiscovery/cvemap/cmd/vulnx@latest")
        sys.exit(1)

    # Check for API authentication
    creds_file = os.path.expanduser("~/.pdcp/credentials.yaml")
    if not os.path.exists(creds_file):
        print("Warning: vulnx not authenticated. You may hit rate limits.")
        print("Get API key from: https://cloud.projectdiscovery.io/")
        print("Authenticate with: vulnx auth --api-key YOUR_KEY")
        print()


def generate_search_key(display_name):
    """
    Generate a search key for vulnx queries.
    Returns a normalized product name suitable for vulnx search.
    """
    name = display_name.lower()

    # Remove common suffixes
    name = re.sub(r'\.js$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'\.net$', '', name, flags=re.IGNORECASE)
    name = re.sub(r' framework$', '', name, flags=re.IGNORECASE)
    name = re.sub(r' auth$', '', name, flags=re.IGNORECASE)
    name = re.sub(r' api$', '', name, flags=re.IGNORECASE)
    name = re.sub(r' spec$', '', name, flags=re.IGNORECASE)
    name = re.sub(r' collection$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'^sign in with ', '', name, flags=re.IGNORECASE)

    # Replace spaces with underscores for multi-word products
    name = name.replace(' ', '_')

    return name.strip()


def save_progress(findings, results):
    """Save progress incrementally to findings.json"""
    for result in results:
        slug = result['slug']
        if slug in findings:
            cve_data = {
                'stats': result['stats'],
                'updated': result['updated']
            }

            # Include search_key if present
            if result.get('search_key'):
                cve_data['search_key'] = result['search_key']
            elif 'security' in findings[slug] and 'cve' in findings[slug]['security']:
                existing_key = findings[slug]['security']['cve'].get('search_key')
                if existing_key:
                    cve_data['search_key'] = existing_key

            # Preserve existing security object and only update cve field
            if 'security' not in findings[slug]:
                findings[slug]['security'] = {}

            findings[slug]['security']['cve'] = cve_data

    # Write updated findings back to file
    with open(FINDINGS_FILE, 'w') as f:
        json.dump(findings, f, indent=2, ensure_ascii=False)
        f.write('\n')


def should_query_finding(finding):
    """Check if a finding should be queried for CVEs based on security.cve_applicable flag."""
    # Check security.cve_applicable field (default is True if not specified)
    security = finding.get('security', {})
    cve_applicable = security.get('cve_applicable', True)

    # If explicitly set to False, skip
    if cve_applicable is False:
        return False

    return True


def get_cve_stats(search_key, slug, update_search_key=False, retry_count=0):
    """Query vulnx for CVE statistics including KEV count."""
    max_retries = 3

    print(f"  Querying vulnx for: {search_key} (slug: {slug})...")

    try:
        # Get current timestamp in ISO 8601 format (UTC)
        timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

        # Query 1: Get total CVE count
        result = subprocess.run(
            ["vulnx", "search", search_key, "--json", "--silent", "--limit", "1"],
            capture_output=True,
            text=True,
            timeout=30,
            env=os.environ.copy()
        )

        # Check for rate limit error
        if "Rate limit exceeded" in result.stderr or "Rate limit exceeded" in result.stdout:
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 10
                print(f"    Rate limit hit. Waiting {wait_time}s before retry {retry_count + 1}/{max_retries}...")
                time.sleep(wait_time)
                return get_cve_stats(search_key, slug, update_search_key, retry_count + 1)
            else:
                print(f"    ERROR: Rate limit exceeded after {max_retries} retries. Skipping.")
                return None

        if not result.stdout.strip():
            print(f"    No CVEs found (checked: {timestamp})")
            return {
                "slug": slug,
                "search_key": search_key if update_search_key else None,
                "stats": {"critical": 0, "high": 0, "medium": 0, "low": 0, "total": 0, "kev": 0},
                "updated": timestamp
            }

        try:
            data = json.loads(result.stdout)
            total = data.get('total', 0)

            if total == 0:
                print(f"    No CVEs found (checked: {timestamp})")
                return {
                    "slug": slug,
                    "search_key": search_key if update_search_key else None,
                    "stats": {"critical": 0, "high": 0, "medium": 0, "low": 0, "total": 0, "kev": 0},
                    "updated": timestamp
                }

            # Query 2-5: Get accurate counts for each severity using filters
            critical = 0
            high = 0
            medium = 0
            low = 0

            # Delay before next query to avoid rate limiting
            time.sleep(QUERY_DELAY)

            # Query critical count
            crit_result = subprocess.run(
                ["vulnx", "search", f"{search_key} && severity:critical", "--json", "--silent", "--limit", "1"],
                capture_output=True, text=True, timeout=30, env=os.environ.copy()
            )
            if crit_result.stdout.strip():
                try:
                    critical = json.loads(crit_result.stdout).get('total', 0)
                except json.JSONDecodeError:
                    pass

            # Delay before next query to avoid rate limiting
            time.sleep(QUERY_DELAY)

            # Query high count
            high_result = subprocess.run(
                ["vulnx", "search", f"{search_key} && severity:high", "--json", "--silent", "--limit", "1"],
                capture_output=True, text=True, timeout=30, env=os.environ.copy()
            )
            if high_result.stdout.strip():
                try:
                    high = json.loads(high_result.stdout).get('total', 0)
                except json.JSONDecodeError:
                    pass

            # Delay before next query to avoid rate limiting
            time.sleep(QUERY_DELAY)

            # Query medium count
            med_result = subprocess.run(
                ["vulnx", "search", f"{search_key} && severity:medium", "--json", "--silent", "--limit", "1"],
                capture_output=True, text=True, timeout=30, env=os.environ.copy()
            )
            if med_result.stdout.strip():
                try:
                    medium = json.loads(med_result.stdout).get('total', 0)
                except json.JSONDecodeError:
                    pass

            # Delay before next query to avoid rate limiting
            time.sleep(QUERY_DELAY)

            # Query low count
            low_result = subprocess.run(
                ["vulnx", "search", f"{search_key} && severity:low", "--json", "--silent", "--limit", "1"],
                capture_output=True, text=True, timeout=30, env=os.environ.copy()
            )
            if low_result.stdout.strip():
                try:
                    low = json.loads(low_result.stdout).get('total', 0)
                except json.JSONDecodeError:
                    pass

            # Delay before next query to avoid rate limiting
            time.sleep(QUERY_DELAY)

            # Query 6: Get KEV count
            kev = 0
            kev_result = subprocess.run(
                ["vulnx", "search", f"{search_key} && is_kev:true", "--json", "--silent", "--limit", "1"],
                capture_output=True, text=True, timeout=30, env=os.environ.copy()
            )
            if kev_result.stdout.strip():
                try:
                    kev = json.loads(kev_result.stdout).get('total', 0)
                except json.JSONDecodeError:
                    pass

            print(f"    Found {total} CVEs: critical={critical}, high={high}, medium={medium}, low={low}, kev={kev}")

            return {
                "slug": slug,
                "search_key": search_key if update_search_key else None,
                "stats": {
                    "critical": critical,
                    "high": high,
                    "medium": medium,
                    "low": low,
                    "total": total,
                    "kev": kev
                },
                "updated": timestamp
            }
        except json.JSONDecodeError as e:
            print(f"    Warning: Could not parse vulnx output for {search_key}: {e}")
            return None

    except subprocess.TimeoutExpired:
        print(f"    Warning: vulnx query timed out for {search_key}")
        return None
    except Exception as e:
        print(f"    Warning: Error querying vulnx for {search_key}: {e}")
        return None


def main():
    print("Starting CVE statistics update with vulnx...")
    print(f"Rate limit delay: {RATE_LIMIT_DELAY}s between findings")
    print(f"Query delay: {QUERY_DELAY}s between queries")
    print()

    # Check for resume option
    resume_from = None
    if len(sys.argv) > 1:
        if sys.argv[1] == '--resume-from' and len(sys.argv) > 2:
            resume_from = sys.argv[2]
            print(f"Resuming from: {resume_from}")
        elif sys.argv[1] in ['-h', '--help']:
            print(__doc__)
            sys.exit(0)

    # Check dependencies
    check_dependencies()

    # Load findings
    if not FINDINGS_FILE.exists():
        print(f"Error: {FINDINGS_FILE} not found")
        sys.exit(1)

    with open(FINDINGS_FILE, 'r') as f:
        findings = json.load(f)

    total_findings = len(findings)
    print(f"Total findings in file: {total_findings}")
    print()

    # Process each finding
    results = []
    count = 0
    processed = 0
    skipped = 0
    resuming = resume_from is not None

    print("Processing findings...")
    for slug, finding in findings.items():
        count += 1
        display_name = finding.get('display_name', slug)

        # Skip until we reach the resume point
        if resuming:
            if slug == resume_from:
                resuming = False
                print(f"[{count}/{total_findings}] Resuming at: {slug} - {display_name}")
            else:
                print(f"[{count}/{total_findings}] Skipping (resume): {slug}")
                skipped += 1
                continue

        print(f"[{count}/{total_findings}] Processing: {slug} - {display_name}")

        # Check if we should query this finding based on cve_applicable flag
        if not should_query_finding(finding):
            print("  Skipping (cve_applicable=false)")
            skipped += 1
            continue

        # Use existing search_key from security.cve if present, otherwise generate one
        search_key = None
        generated_search_key = False
        if 'security' in finding and 'cve' in finding['security']:
            search_key = finding['security']['cve'].get('search_key')

        if not search_key:
            search_key = generate_search_key(display_name)
            generated_search_key = True
            print(f"  Generated search_key: {search_key}")
        else:
            print(f"  Using existing search_key: {search_key}")

        # Skip if search key is too generic or empty
        if not search_key or search_key == '-':
            print("  Skipping (invalid search key)")
            skipped += 1
            continue

        # Get CVE stats (only update search_key if we generated a new one)
        stats = get_cve_stats(search_key, slug, update_search_key=generated_search_key)
        if stats:
            results.append(stats)
            processed += 1

            # Save progress incrementally every 5 findings
            if processed > 0 and processed % 5 == 0:
                print(f"  ðŸ’¾ Saving progress ({processed} findings processed)...")
                save_progress(findings, results)

        # Delay to avoid rate limiting
        time.sleep(RATE_LIMIT_DELAY)

    print()
    print(f"Processed: {processed} findings")
    print(f"Skipped: {skipped} findings")
    print()

    if not results:
        print("No products queried (all findings were skipped)")
        return

    # Update findings with CVE statistics
    print("Updating findings.json with CVE statistics...")
    for result in results:
        slug = result['slug']
        if slug in findings:
            cve_data = {
                'stats': result['stats'],
                'updated': result['updated']
            }

            # Include search_key: either from result (if newly generated) or preserve existing one
            if result['search_key'] is not None:
                # Newly generated search_key
                cve_data['search_key'] = result['search_key']
            elif 'security' in findings[slug] and 'cve' in findings[slug]['security']:
                # Preserve existing search_key
                existing_key = findings[slug]['security']['cve'].get('search_key')
                if existing_key:
                    cve_data['search_key'] = existing_key

            # Preserve existing security object and only update cve field
            if 'security' not in findings[slug]:
                findings[slug]['security'] = {}

            findings[slug]['security']['cve'] = cve_data

    # Write updated findings back to file
    with open(FINDINGS_FILE, 'w') as f:
        json.dump(findings, f, indent=2, ensure_ascii=False)
        f.write('\n')  # Add trailing newline

    print()
    print("âœ“ CVE statistics updated successfully!")
    print()
    print("Summary:")
    for slug, finding in findings.items():
        if 'security' in finding and 'cve' in finding['security']:
            cve = finding['security']['cve']
            stats = cve['stats']
            search_key = cve.get('search_key', 'N/A')
            print(f"{slug} (search: {search_key}): "
                  f"critical={stats['critical']}, high={stats['high']}, "
                  f"medium={stats['medium']}, low={stats['low']}, "
                  f"kev={stats['kev']}, total={stats['total']} "
                  f"(updated: {cve['updated']})")


if __name__ == '__main__':
    main()
